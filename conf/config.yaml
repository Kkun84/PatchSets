defaults:
  - optimizer: Adam
  - lr_scheduler: Step

name: ''
seed: ~

hparams:
  batch_size: 1024
  num_workers: 4
  lr: 0.001
  train_patch_n: [1, 2, 4, 8, 16]
  test_patch_n: [1, 2, 4, 8, 16]
  patch_size: 8

model_params:
  encoder:
    input_shape:
      - 1
      - ${hparams.patch_size}
      - ${hparams.patch_size}
    hidden_n_0: 64
    hidden_n_1: 64
    output_n: 64
  decoder:
    input_n: 64
    hidden_n_0: 64
    hidden_n_1: 64
    output_n: 10

optim:
  optimizer: ${optimizer}
  # lr_scheduler: ${lr_scheduler}
  lr_scheduler: ~

trainer:
  gpus: 1
  num_nodes: 1

callback:
  checkpoint:
    class: pytorch_lightning.callbacks.ModelCheckpoint
    params:
      filepath: ~
      monitor: val_loss
      verbose: False
      save_top_k: 3
      mode: auto
  early_stopping:
    class: pytorch_lightning.callbacks.EarlyStopping
    params:
      monitor: val_loss
      min_delta: 0
      patience: 100
      verbose: False
      mode: auto
  callbacks:
    ~

loggers:
  - class: pytorch_lightning.loggers.TensorBoardLogger
    params:
      save_dir: '.'
      name: ''
      version: ''
  - class: pytorch_lightning.loggers.MLFlowLogger
    params:
      experiment_name: Default
      tracking_uri: "file:/workspace/mlruns"
      tags: ~
      save_dir: ~
