defaults:
  - optimizer: Adam
  - lr_scheduler: ~
  - dataset: MNIST

name: ''
experiment_name: Default

hparams:
  batch_size: 1024
  num_workers: 6
  lr: 0.001
  train_patch_n: [1, 2, 4, 8, 16, 32]
  test_patch_n: [1, 2, 4, 8, 16, 32, 64, 128]
  patch_size: 5
  latent_dim: 64
  n_pow: 1
  dataset_n_splits: 5
  dataset_n: 0
  early_stopping: 100
  seed: 0

model_params:
  encoder:
    input_shape:
      - 1
      - ${hparams.patch_size}
      - ${hparams.patch_size}
    hidden_n_0: 64
    hidden_n_1: 64
    output_n: ${hparams.latent_dim}
    n_pow: ${hparams.n_pow}
  decoder:
    input_n: ${hparams.latent_dim}
    hidden_n_0: 64
    hidden_n_1: 64
    output_n: 10

optim:
  optimizer: ${optimizer}
  lr_scheduler: ~

trainer:
  gpus: 1
  num_nodes: 1
  deterministic: False

callback:
  checkpoint:
    class: pytorch_lightning.callbacks.ModelCheckpoint
    params:
      filepath: ~
      monitor: val_loss
      verbose: False
      save_top_k: 3
      mode: auto
  early_stopping:
    class: pytorch_lightning.callbacks.EarlyStopping
    params:
      monitor: val_loss
      min_delta: 0
      patience: ${hparams.early_stopping}
      verbose: False
      mode: auto
  callbacks:
    ~

loggers:
  - class: pytorch_lightning.loggers.TensorBoardLogger
    params:
      save_dir: '.'
      name: ''
      version: ''
  - class: pytorch_lightning.loggers.MLFlowLogger
    params:
      experiment_name: ${experiment_name}
      tracking_uri: "file:/workspace/mlruns"
      tags: ~
      save_dir: ~
